{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **MAS 651 Midterm Exam (40 points)**\n",
    "\n",
    "Philip Bachas-Daunert"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e720a0a75052ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 1 (4 Points)**\n",
    "\n",
    "Find out how many rows and columns this data set has. Check if the data set has any missing value. Remove missing values if there are any. Show the first 10 observations of this data set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5a0f5eab2827fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 1 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db50375e4e92676"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10000\n",
      "Number of columns: 14\n",
      "There are no missing values in the dataset.\n",
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "5          6    15574012       Chu          645     Spain    Male   44   \n",
      "6          7    15592531  Bartlett          822    France    Male   50   \n",
      "7          8    15656148    Obinna          376   Germany  Female   29   \n",
      "8          9    15792365        He          501    France    Male   44   \n",
      "9         10    15592389        H?          684    France    Male   27   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "3       1       0.00              2          0               0   \n",
      "4       2  125510.82              1          1               1   \n",
      "5       8  113755.78              2          1               0   \n",
      "6       7       0.00              2          1               1   \n",
      "7       4  115046.74              4          1               0   \n",
      "8       4  142051.07              2          0               1   \n",
      "9       2  134603.88              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "5        149756.71       1  \n",
      "6         10062.80       0  \n",
      "7        119346.88       1  \n",
      "8         74940.50       0  \n",
      "9         71725.73       0  \n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "churn_data = pd.read_csv('https://raw.githubusercontent.com/wangx346/MAS651/main/Churn.csv')\n",
    "\n",
    "# Display the shape of the DataFrame to understand its size\n",
    "num_rows, num_columns = churn_data.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "# Check for the presence of any missing values within the DataFrame\n",
    "if churn_data.isnull().values.any():\n",
    "    print(\"There are missing values in the dataset.\")\n",
    "    \n",
    "    # Remove rows with any missing values to ensure data integrity\n",
    "    churn_data.dropna(inplace=True)\n",
    "    \n",
    "    # Display the new shape after removing missing values\n",
    "    new_num_rows = churn_data.shape[0]\n",
    "    print(f\"New number of rows after removing missing values: {new_num_rows}\")\n",
    "else:\n",
    "    print(\"There are no missing values in the dataset.\")\n",
    "\n",
    "# Display the first 10 observations of the DataFrame\n",
    "print(churn_data.head(10))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:58:17.656475Z",
     "start_time": "2024-02-12T16:58:17.114434Z"
    }
   },
   "id": "52042b00bafa5dbc",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 2 (4 Points)**\n",
    "\n",
    "Which 3 columns should not be used as features? Exclude these columns. Create the appropriate X matrix (representing categorical variables using dummy variables). Define the y vector as the last column of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e3d6875e9f8ba78"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 2 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb725791935e4c5e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclude the columns that should not be used as features: ['RowNumber', 'CustomerId', 'Surname']. This is because they are identifiers and not relevant to the prediction.\n",
      "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
      "0          619   42       2       0.00              1          1   \n",
      "1          608   41       1   83807.86              1          0   \n",
      "2          502   42       8  159660.80              3          1   \n",
      "3          699   39       1       0.00              2          0   \n",
      "4          850   43       2  125510.82              1          1   \n",
      "\n",
      "   IsActiveMember  EstimatedSalary  Geography_France  Geography_Germany  \\\n",
      "0               1        101348.88              True              False   \n",
      "1               1        112542.58             False              False   \n",
      "2               0        113931.57              True              False   \n",
      "3               0         93826.63              True              False   \n",
      "4               1         79084.10             False              False   \n",
      "\n",
      "   Geography_Spain  Gender_Female  Gender_Male  \n",
      "0            False           True        False  \n",
      "1             True           True        False  \n",
      "2            False           True        False  \n",
      "3            False           True        False  \n",
      "4             True           True        False  \n",
      "0    1\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: Exited, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Exclude the columns that should not be used as features\n",
    "columns_to_exclude = ['RowNumber', 'CustomerId', 'Surname']\n",
    "\n",
    "print(f\"Exclude the columns that should not be used as features: {columns_to_exclude}. This is because they are identifiers and not relevant to the prediction.\") \n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "churn_data_filtered = churn_data.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "categorical_columns = ['Geography', 'Gender']\n",
    "churn_data_with_dummies = pd.get_dummies(churn_data_filtered, columns=categorical_columns)\n",
    "\n",
    "# Define the X matrix (features) and y vector (target variable is last column)\n",
    "X = churn_data_with_dummies.drop('Exited', axis=1)\n",
    "y = churn_data_with_dummies['Exited']\n",
    "\n",
    "# Show the first few rows of X to verify the transformation\n",
    "print(X.head())\n",
    "\n",
    "# Show the first few rows of y to verify the target variable\n",
    "print(y.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:58:17.658215Z",
     "start_time": "2024-02-12T16:58:17.652605Z"
    }
   },
   "id": "60f6ad2070ea1ab6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 3 (6 Points)**\n",
    "\n",
    "Create a training data set and a test data set such that the test data set contains randomly 20% of the data set. Set the random seed to be 40. Scale the features in both the training and testing set using StandardScaler."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17f946801eb7d851"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 3 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805b21186d88c957"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Set shape: (8000, 13)\n",
      "Test Data Set shape: (2000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data with the same scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Show the shapes of the resulting datasets to confirm the split\n",
    "print(f\"Training Data Set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test Data Set shape: {X_test_scaled.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:58:18.017205Z",
     "start_time": "2024-02-12T16:58:17.658976Z"
    }
   },
   "id": "b607a5e93366bdfa",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 4 (6 Points)**\n",
    "\n",
    "Implement Xgboost for classification on the training data. Evaluate the performance of the Xgboost classifier on the test data. Report accuracy and the confusion matrix. Report precision and recall."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52e2dd284edef7a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 4 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279da473b25da9b7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "----------------------------\n",
      "Accuracy : 0.8735\n",
      "Precision: 0.7162\n",
      "Recall   : 0.5651\n",
      "\n",
      "Confusion Matrix:\n",
      "1530 (True Negative)  | 86 (False Positive)\n",
      "167 (False Negative) | 217 (True Positive)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Fit the classifier to the scaled training data\n",
    "xgb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = xgb_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy, confusion matrix, precision, and recall\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Evaluation Metrics:\\n{'-'*28}\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(f\"{conf_matrix[0][0]} (True Negative)  | {conf_matrix[0][1]} (False Positive)\")\n",
    "print(f\"{conf_matrix[1][0]} (False Negative) | {conf_matrix[1][1]} (True Positive)\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:58:18.283876Z",
     "start_time": "2024-02-12T16:58:18.014889Z"
    }
   },
   "id": "9fce09e0283c3a11",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 5 (5 Points)**\n",
    "\n",
    "Could you find better choices of parameters using GridSearchCV? Does the performance improve with this choice of parameter?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "213291cd9afd1c7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 5 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bccce67e385b7184"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters found: {'learning_rate': 0.01, 'max_depth': 8, 'n_estimators': 300}\n",
      "\n",
      "Comparison of Original and Optimized Model Performances:\n",
      "------------------------------------------------------------\n",
      "Metric      Original  Optimized \n",
      "Accuracy    0.8735    0.8740\n",
      "Precision   0.7162    0.7391\n",
      "Recall      0.5651    0.5312\n",
      "\n",
      "Original Confusion Matrix:\n",
      "1530 (TN) | 86 (FP)\n",
      "167 (FN) | 217 (TP)\n",
      "\n",
      "Optimized Confusion Matrix:\n",
      "1544 (TN) | 72 (FP)\n",
      "180 (FN) | 204 (TP)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_classifier = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Initialize GridSearchCV with the parameter grid, classifier, and scoring metric\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, scoring='accuracy', cv=5, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator found to predict the labels of the test set\n",
    "y_pred_optimized = grid_search.best_estimator_.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print the improved accuracy, confusion matrix, precision, and recall\n",
    "accuracy_optimized = accuracy_score(y_test, y_pred_optimized)\n",
    "conf_matrix_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "precision_optimized = precision_score(y_test, y_pred_optimized)\n",
    "recall_optimized = recall_score(y_test, y_pred_optimized)\n",
    "\n",
    "# Function to compare model performance\n",
    "def compare_model_performance(accuracy_orig, precision_orig, recall_orig, conf_matrix_orig,\n",
    "                              accuracy_opt, precision_opt, recall_opt, conf_matrix_opt):\n",
    "    \"\"\"\n",
    "    Compares the performance metrics of the original XGBoost model and the optimized model.\n",
    "    \"\"\"\n",
    "    print(\"\\nComparison of Original and Optimized Model Performances:\\n\" + \"-\"*60)\n",
    "    print(f\"{'Metric':<12}{'Original':<10}{'Optimized':<10}\")\n",
    "    print(f\"{'Accuracy':<12}{accuracy_orig:.4f}    {accuracy_opt:.4f}\")\n",
    "    print(f\"{'Precision':<12}{precision_orig:.4f}    {precision_opt:.4f}\")\n",
    "    print(f\"{'Recall':<12}{recall_orig:.4f}    {recall_opt:.4f}\\n\")\n",
    "    \n",
    "    print(\"Original Confusion Matrix:\")\n",
    "    print(f\"{conf_matrix_orig[0][0]} (TN) | {conf_matrix_orig[0][1]} (FP)\")\n",
    "    print(f\"{conf_matrix_orig[1][0]} (FN) | {conf_matrix_orig[1][1]} (TP)\\n\")\n",
    "    \n",
    "    print(\"Optimized Confusion Matrix:\")\n",
    "    print(f\"{conf_matrix_opt[0][0]} (TN) | {conf_matrix_opt[0][1]} (FP)\")\n",
    "    print(f\"{conf_matrix_opt[1][0]} (FN) | {conf_matrix_opt[1][1]} (TP)\")\n",
    "\n",
    "# Call the function with both the original and optimized metrics\n",
    "compare_model_performance(accuracy, precision, recall, conf_matrix,\n",
    "                          accuracy_optimized, precision_optimized, recall_optimized, conf_matrix_optimized)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:59:03.687311Z",
     "start_time": "2024-02-12T16:58:18.284338Z"
    }
   },
   "id": "b07bdc5ec920c581",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 6 (5 Points)**\n",
    "\n",
    "How does the accuracy of Xgboost compare with that of logistic regression? Show your comparisons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7ee36889c3ec853"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 6 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae226666f2e338e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Model Accuracies:\n",
      "-----------------------------------\n",
      "Model               Accuracy       \n",
      "Logistic Regression 0.8210\n",
      "Optimized XGBoost   0.8740\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model to the scaled training data\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred_log_reg = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the logistic regression model\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "\n",
    "# Function to compare the accuracy of two models\n",
    "def compare_accuracy(accuracy_xgb, accuracy_lr):\n",
    "    \"\"\"\n",
    "    Compares the accuracy of the XGBoost model and the Logistic Regression model.\n",
    "    \"\"\"\n",
    "    print(\"\\nComparison of Model Accuracies:\\n\" + \"-\"*35)\n",
    "    print(f\"{'Model':<20}{'Accuracy':<15}\")\n",
    "    print(f\"{'Logistic Regression':<20}{accuracy_lr:.4f}\")\n",
    "    print(f\"{'Optimized XGBoost':<20}{accuracy_xgb:.4f}\")\n",
    "\n",
    "# Call the function to compare the accuracies\n",
    "compare_accuracy(accuracy_optimized, accuracy_log_reg)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:59:03.838665Z",
     "start_time": "2024-02-12T16:59:03.683837Z"
    }
   },
   "id": "21cceb4dbbc45f5a",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 7 (10 Points)**\n",
    "\n",
    "Fit an artificial neural network model with two hidden layers to predict the customer churn. Use activation='sigmoid' for the output layer. How does the accuracy of the neural network model compare with Xgboost and logistic regression?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f04b705b752a756"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Question 7 Answer**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92a3e2dc32412b60"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-12 11:59:05.338867: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8834 - accuracy: 0.5444 - val_loss: 0.6915 - val_accuracy: 0.5288 - lr: 1.0000e-04\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.7812 - accuracy: 0.5889 - val_loss: 0.6525 - val_accuracy: 0.6281 - lr: 1.0000e-04\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.7246 - accuracy: 0.6270 - val_loss: 0.5861 - val_accuracy: 0.7144 - lr: 1.0000e-04\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.6403 - val_loss: 0.5390 - val_accuracy: 0.7456 - lr: 1.0000e-04\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6588 - accuracy: 0.6594 - val_loss: 0.4991 - val_accuracy: 0.7769 - lr: 1.0000e-04\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6425 - accuracy: 0.6783 - val_loss: 0.4708 - val_accuracy: 0.8025 - lr: 1.0000e-04\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6099 - accuracy: 0.6984 - val_loss: 0.4520 - val_accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5699 - accuracy: 0.7203 - val_loss: 0.4362 - val_accuracy: 0.8194 - lr: 1.0000e-04\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7241 - val_loss: 0.4217 - val_accuracy: 0.8306 - lr: 1.0000e-04\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.7364 - val_loss: 0.4090 - val_accuracy: 0.8331 - lr: 1.0000e-04\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5343 - accuracy: 0.7472 - val_loss: 0.3979 - val_accuracy: 0.8363 - lr: 1.0000e-04\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5171 - accuracy: 0.7652 - val_loss: 0.3896 - val_accuracy: 0.8381 - lr: 1.0000e-04\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 0s 999us/step - loss: 0.5139 - accuracy: 0.7639 - val_loss: 0.3832 - val_accuracy: 0.8406 - lr: 1.0000e-04\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7713 - val_loss: 0.3756 - val_accuracy: 0.8487 - lr: 1.0000e-04\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.7742 - val_loss: 0.3721 - val_accuracy: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.7769 - val_loss: 0.3679 - val_accuracy: 0.8500 - lr: 1.0000e-04\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.7845 - val_loss: 0.3635 - val_accuracy: 0.8494 - lr: 1.0000e-04\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7928 - val_loss: 0.3610 - val_accuracy: 0.8537 - lr: 1.0000e-04\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4756 - accuracy: 0.7847 - val_loss: 0.3577 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4700 - accuracy: 0.7902 - val_loss: 0.3557 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.8027 - val_loss: 0.3526 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8023 - val_loss: 0.3513 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 0s 997us/step - loss: 0.4586 - accuracy: 0.7959 - val_loss: 0.3490 - val_accuracy: 0.8581 - lr: 1.0000e-04\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7972 - val_loss: 0.3474 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8109 - val_loss: 0.3459 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 0s 975us/step - loss: 0.4410 - accuracy: 0.8083 - val_loss: 0.3448 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 0s 967us/step - loss: 0.4419 - accuracy: 0.8086 - val_loss: 0.3427 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.4350 - accuracy: 0.8161 - val_loss: 0.3418 - val_accuracy: 0.8581 - lr: 1.0000e-04\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 0s 926us/step - loss: 0.4344 - accuracy: 0.8152 - val_loss: 0.3407 - val_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.4361 - accuracy: 0.8136 - val_loss: 0.3400 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 0s 948us/step - loss: 0.4293 - accuracy: 0.8134 - val_loss: 0.3385 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 0s 915us/step - loss: 0.4313 - accuracy: 0.8139 - val_loss: 0.3377 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.4234 - accuracy: 0.8181 - val_loss: 0.3378 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 0s 947us/step - loss: 0.4285 - accuracy: 0.8197 - val_loss: 0.3372 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 0s 930us/step - loss: 0.4134 - accuracy: 0.8209 - val_loss: 0.3364 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 0s 931us/step - loss: 0.4247 - accuracy: 0.8194 - val_loss: 0.3358 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.4119 - accuracy: 0.8286 - val_loss: 0.3352 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 0s 964us/step - loss: 0.4127 - accuracy: 0.8250 - val_loss: 0.3355 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 0s 945us/step - loss: 0.4142 - accuracy: 0.8213 - val_loss: 0.3356 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 0s 983us/step - loss: 0.4090 - accuracy: 0.8258 - val_loss: 0.3350 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 0s 947us/step - loss: 0.4134 - accuracy: 0.8242 - val_loss: 0.3346 - val_accuracy: 0.8600 - lr: 1.0000e-04\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 0s 944us/step - loss: 0.4085 - accuracy: 0.8308 - val_loss: 0.3343 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 0s 930us/step - loss: 0.4104 - accuracy: 0.8289 - val_loss: 0.3347 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 0s 955us/step - loss: 0.4125 - accuracy: 0.8245 - val_loss: 0.3338 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 0s 961us/step - loss: 0.4064 - accuracy: 0.8270 - val_loss: 0.3335 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 0s 956us/step - loss: 0.4090 - accuracy: 0.8261 - val_loss: 0.3334 - val_accuracy: 0.8562 - lr: 1.0000e-04\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.4073 - accuracy: 0.8255 - val_loss: 0.3326 - val_accuracy: 0.8569 - lr: 1.0000e-04\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 0s 915us/step - loss: 0.4039 - accuracy: 0.8277 - val_loss: 0.3324 - val_accuracy: 0.8612 - lr: 1.0000e-04\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 0s 935us/step - loss: 0.4019 - accuracy: 0.8250 - val_loss: 0.3325 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 0s 991us/step - loss: 0.4068 - accuracy: 0.8306 - val_loss: 0.3318 - val_accuracy: 0.8612 - lr: 1.0000e-04\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 0s 964us/step - loss: 0.4032 - accuracy: 0.8300 - val_loss: 0.3314 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 0s 980us/step - loss: 0.3892 - accuracy: 0.8342 - val_loss: 0.3316 - val_accuracy: 0.8594 - lr: 1.0000e-04\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8341 - val_loss: 0.3315 - val_accuracy: 0.8581 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.3929 - accuracy: 0.8339 - val_loss: 0.3308 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 0s 956us/step - loss: 0.3947 - accuracy: 0.8314 - val_loss: 0.3311 - val_accuracy: 0.8575 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8355 - val_loss: 0.3308 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8322 - val_loss: 0.3311 - val_accuracy: 0.8587 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8359 - val_loss: 0.3307 - val_accuracy: 0.8619 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8320 - val_loss: 0.3303 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "100/100 [==============================] - 0s 992us/step - loss: 0.3954 - accuracy: 0.8313 - val_loss: 0.3306 - val_accuracy: 0.8631 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "100/100 [==============================] - 0s 962us/step - loss: 0.3922 - accuracy: 0.8361 - val_loss: 0.3306 - val_accuracy: 0.8606 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "100/100 [==============================] - 0s 990us/step - loss: 0.3902 - accuracy: 0.8309 - val_loss: 0.3304 - val_accuracy: 0.8637 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8364 - val_loss: 0.3306 - val_accuracy: 0.8619 - lr: 1.0000e-04\n",
      "Epoch 64/300\n",
      " 59/100 [================>.............] - ETA: 0s - loss: 0.3873 - accuracy: 0.8411\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8411 - val_loss: 0.3303 - val_accuracy: 0.8619 - lr: 1.0000e-04\n",
      "Epoch 65/300\n",
      "100/100 [==============================] - 0s 995us/step - loss: 0.3855 - accuracy: 0.8342 - val_loss: 0.3302 - val_accuracy: 0.8637 - lr: 2.0000e-05\n",
      "Epoch 66/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8381 - val_loss: 0.3301 - val_accuracy: 0.8619 - lr: 2.0000e-05\n",
      "Epoch 67/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8392 - val_loss: 0.3302 - val_accuracy: 0.8625 - lr: 2.0000e-05\n",
      "Epoch 68/300\n",
      "100/100 [==============================] - 0s 987us/step - loss: 0.3841 - accuracy: 0.8377 - val_loss: 0.3302 - val_accuracy: 0.8619 - lr: 2.0000e-05\n",
      "Epoch 69/300\n",
      "100/100 [==============================] - 0s 997us/step - loss: 0.3885 - accuracy: 0.8378 - val_loss: 0.3301 - val_accuracy: 0.8619 - lr: 2.0000e-05\n",
      "Epoch 70/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8361 - val_loss: 0.3301 - val_accuracy: 0.8625 - lr: 2.0000e-05\n",
      "Epoch 71/300\n",
      " 60/100 [=================>............] - ETA: 0s - loss: 0.3817 - accuracy: 0.8414\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.8358 - val_loss: 0.3301 - val_accuracy: 0.8619 - lr: 2.0000e-05\n",
      "Epoch 72/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8366 - val_loss: 0.3300 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 73/300\n",
      "100/100 [==============================] - 0s 985us/step - loss: 0.3882 - accuracy: 0.8350 - val_loss: 0.3300 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 74/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8377 - val_loss: 0.3301 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
      "Epoch 75/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8386 - val_loss: 0.3300 - val_accuracy: 0.8619 - lr: 1.0000e-05\n",
      "Epoch 76/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3888 - accuracy: 0.8378 - val_loss: 0.3299 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 77/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8373 - val_loss: 0.3299 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 78/300\n",
      "100/100 [==============================] - 0s 996us/step - loss: 0.3867 - accuracy: 0.8355 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 79/300\n",
      "100/100 [==============================] - 0s 939us/step - loss: 0.3832 - accuracy: 0.8350 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 80/300\n",
      "100/100 [==============================] - 0s 977us/step - loss: 0.3922 - accuracy: 0.8358 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 81/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8381 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 82/300\n",
      "100/100 [==============================] - 0s 974us/step - loss: 0.3856 - accuracy: 0.8366 - val_loss: 0.3297 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 83/300\n",
      "100/100 [==============================] - 0s 954us/step - loss: 0.3822 - accuracy: 0.8355 - val_loss: 0.3297 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 0s 945us/step - loss: 0.3895 - accuracy: 0.8313 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 0s 948us/step - loss: 0.3815 - accuracy: 0.8381 - val_loss: 0.3297 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 0s 937us/step - loss: 0.3846 - accuracy: 0.8355 - val_loss: 0.3297 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 0s 964us/step - loss: 0.3812 - accuracy: 0.8413 - val_loss: 0.3296 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.3849 - accuracy: 0.8364 - val_loss: 0.3298 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 0s 940us/step - loss: 0.3896 - accuracy: 0.8350 - val_loss: 0.3297 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 0s 983us/step - loss: 0.3862 - accuracy: 0.8348 - val_loss: 0.3297 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 0s 950us/step - loss: 0.3850 - accuracy: 0.8372 - val_loss: 0.3298 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 0s 971us/step - loss: 0.3847 - accuracy: 0.8369 - val_loss: 0.3297 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.3914 - accuracy: 0.8341 - val_loss: 0.3296 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 0s 956us/step - loss: 0.3940 - accuracy: 0.8323 - val_loss: 0.3295 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 0s 965us/step - loss: 0.3823 - accuracy: 0.8353 - val_loss: 0.3294 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8356 - val_loss: 0.3295 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 0s 984us/step - loss: 0.3863 - accuracy: 0.8338 - val_loss: 0.3295 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 0s 949us/step - loss: 0.3907 - accuracy: 0.8316 - val_loss: 0.3295 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 0s 952us/step - loss: 0.3850 - accuracy: 0.8384 - val_loss: 0.3295 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 0s 947us/step - loss: 0.3868 - accuracy: 0.8388 - val_loss: 0.3294 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 0s 943us/step - loss: 0.3906 - accuracy: 0.8347 - val_loss: 0.3295 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.3861 - accuracy: 0.8375 - val_loss: 0.3294 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 0s 958us/step - loss: 0.3838 - accuracy: 0.8423 - val_loss: 0.3295 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 0s 957us/step - loss: 0.3770 - accuracy: 0.8367 - val_loss: 0.3293 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 0s 941us/step - loss: 0.3819 - accuracy: 0.8367 - val_loss: 0.3294 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 0s 971us/step - loss: 0.3853 - accuracy: 0.8383 - val_loss: 0.3293 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.3825 - accuracy: 0.8377 - val_loss: 0.3293 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 0s 962us/step - loss: 0.3889 - accuracy: 0.8348 - val_loss: 0.3294 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 0s 968us/step - loss: 0.3855 - accuracy: 0.8391 - val_loss: 0.3293 - val_accuracy: 0.8625 - lr: 1.0000e-05\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 0s 947us/step - loss: 0.3840 - accuracy: 0.8411 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 0s 958us/step - loss: 0.3813 - accuracy: 0.8409 - val_loss: 0.3293 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.3879 - accuracy: 0.8364 - val_loss: 0.3294 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 0s 930us/step - loss: 0.3839 - accuracy: 0.8389 - val_loss: 0.3294 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 0s 991us/step - loss: 0.3832 - accuracy: 0.8417 - val_loss: 0.3293 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3838 - accuracy: 0.8394 - val_loss: 0.3293 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3806 - accuracy: 0.8383 - val_loss: 0.3291 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8384 - val_loss: 0.3291 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3840 - accuracy: 0.8384 - val_loss: 0.3291 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8369 - val_loss: 0.3291 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 0s 995us/step - loss: 0.3809 - accuracy: 0.8441 - val_loss: 0.3292 - val_accuracy: 0.8637 - lr: 1.0000e-05\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8380 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3867 - accuracy: 0.8367 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8367 - val_loss: 0.3293 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8381 - val_loss: 0.3291 - val_accuracy: 0.8644 - lr: 1.0000e-05\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8395 - val_loss: 0.3293 - val_accuracy: 0.8625 - lr: 1.0000e-05\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8356 - val_loss: 0.3291 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8352 - val_loss: 0.3292 - val_accuracy: 0.8625 - lr: 1.0000e-05\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8338 - val_loss: 0.3294 - val_accuracy: 0.8619 - lr: 1.0000e-05\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8355 - val_loss: 0.3293 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 130/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8384 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8395 - val_loss: 0.3292 - val_accuracy: 0.8625 - lr: 1.0000e-05\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8378 - val_loss: 0.3291 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 0s 998us/step - loss: 0.3816 - accuracy: 0.8388 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 0.3292 - val_accuracy: 0.8631 - lr: 1.0000e-05\n",
      "63/63 [==============================] - 0s 293us/step\n",
      "\n",
      "Comparison of Model Accuracies:\n",
      "----------------------------------------\n",
      "Model                    Accuracy       \n",
      "Logistic Regression      0.8210\n",
      "Optimized XGBoost        0.8740\n",
      "Neural Network           0.8645\n",
      "XGBoost has demonstrated to be the most accurate model in this comparison. This is likely because XGBoost is particularly effective with tabular data, leveraging its gradient boosting framework to handle various data irregularities (such as missing values and outliers). On the other hand, while neural networks are powerful for capturing complex patterns and interactions, they may require more extensive data preprocessing, feature engineering, and hyperparameter tuning to achieve optimal performance on tabular datasets.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),  # Increased dropout rate\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),  # Increased dropout rate\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),  # Adjusted learning rate\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add ReduceLROnPlateau to reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Adjust early stopping patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=300, batch_size=64, verbose=1,\n",
    "                    validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "y_pred_nn = np.round(y_pred_nn).flatten()\n",
    "\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "\n",
    "# Function to compare the accuracy of three models\n",
    "def compare_model_accuracies(accuracy_xgb, accuracy_lr, accuracy_nn):\n",
    "    \"\"\"\n",
    "    Compares the accuracy of the XGBoost model, Logistic Regression model, and Neural Network model.\n",
    "    \"\"\"\n",
    "    print(\"\\nComparison of Model Accuracies:\\n\" + \"-\"*40)\n",
    "    print(f\"{'Model':<25}{'Accuracy':<15}\")\n",
    "    print(f\"{'Logistic Regression':<25}{accuracy_lr:.4f}\")\n",
    "    print(f\"{'Optimized XGBoost':<25}{accuracy_xgb:.4f}\")\n",
    "    print(f\"{'Neural Network':<25}{accuracy_nn:.4f}\")\n",
    "\n",
    "# Call the function to compare the accuracies\n",
    "compare_model_accuracies(accuracy_optimized, accuracy_log_reg, accuracy_nn)\n",
    "\n",
    "print(\"XGBoost has demonstrated to be the most accurate model in this comparison. This is likely because XGBoost is particularly effective with tabular data, leveraging its gradient boosting framework to handle various data irregularities (such as missing values and outliers). On the other hand, while neural networks are powerful for capturing complex patterns and interactions, they may require more extensive data preprocessing, feature engineering, and hyperparameter tuning to achieve optimal performance on tabular datasets.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T16:59:19.155593Z",
     "start_time": "2024-02-12T16:59:03.841487Z"
    }
   },
   "id": "b7817af64711a74",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
